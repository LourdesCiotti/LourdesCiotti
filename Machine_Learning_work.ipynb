{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Machine Learning - TP 1**\n",
        "\n",
        "**IMPORT DATA**\n"
      ],
      "metadata": {
        "id": "53inIGZCPmkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from fastai.tabular.all import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "import matplotlib.dates as mdates\n",
        "import glob\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#definimos ruta\n",
        "ruta_store = '/content/drive/My Drive/rossmann-store-sales/store.csv'\n",
        "ruta_train = '/content/drive/My Drive/rossmann-store-sales/train.csv'\n",
        "ruta_test = '/content/drive/My Drive/rossmann-store-sales/test.csv'\n",
        "\n",
        "#ruta_store = '/content/drive/My Drive/MiM/Modulo 2/ML/TP/store.csv'\n",
        "#ruta_train = '/content/drive/My Drive/MiM/Modulo 2/ML/TP/train.csv'\n",
        "#ruta_test = '/content/drive/My Drive/MiM/Modulo 2/ML/TP/test.csv'\n",
        "\n",
        "\n",
        "#importamos archivos\n",
        "store = pd.read_csv(ruta_store)\n",
        "train = pd.read_csv(ruta_train, parse_dates=[2])\n",
        "test = pd.read_csv(ruta_test, parse_dates=[3])\n",
        "\n",
        "# Configurar pandas para mostrar todas las columnas sin truncar y evitar que se envuelvan las filas.\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "\n"
      ],
      "metadata": {
        "id": "EOCqVEjlSoyn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad08076-4095-4e49-b686-c3d49eed658d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-d4e6030c44e3>:35: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train = pd.read_csv(ruta_train, parse_dates=[2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importamos Data Externa\n",
        "ruta_weather = '/content/drive/My Drive/rossmann-store-sales/Weather/*.csv'\n",
        "ruta_state = '/content/drive/My Drive/rossmann-store-sales/store_states.csv'\n",
        "ruta_google_trend = '/content/drive/My Drive/rossmann-store-sales/Google Trend/*.csv'\n",
        "\n",
        "#Clima\n",
        "tiempo_de_los_estados_alemanes = glob.glob(ruta_weather)\n",
        "\n",
        "#Vista previa de archivo meteorológico\n",
        "\n",
        "pd.read_csv(tiempo_de_los_estados_alemanes[0],delimiter=\";\").head()\n",
        "\n",
        "#Estado\n",
        "store_state = pd.read_csv(ruta_state)\n",
        "store_state.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Fx0IsxDUys7n",
        "outputId": "1b30f7b4-bf37-4469-8558-811a3dd4c10c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Store State\n",
              "0      1    HE\n",
              "1      2    TH\n",
              "2      3    NW\n",
              "3      4    BE\n",
              "4      5    SN"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-7a043798-9d37-4630-abda-92c0ff596750\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>State</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>HE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>TH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NW</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>BE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>SN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a043798-9d37-4630-abda-92c0ff596750')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-1e97c74f-9d81-4dbb-a215-32b8ebdc5a0d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1e97c74f-9d81-4dbb-a215-32b8ebdc5a0d')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-1e97c74f-9d81-4dbb-a215-32b8ebdc5a0d button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7a043798-9d37-4630-abda-92c0ff596750 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7a043798-9d37-4630-abda-92c0ff596750');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funciones auxiliares"
      ],
      "metadata": {
        "id": "lKIwPA5owwST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## La dejo acá, podemos sumar otras que no querramos que interrumpa el codigo acá.\n",
        "def graficar(var_predictora, var_predecir, plot_type, plot_title, order=None):\n",
        "    # Generate the plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    if plot_type == 'line':\n",
        "        sns.lineplot(data=train, x=var_predictora, y=var_predecir)\n",
        "    elif plot_type == 'bar':\n",
        "        sns.barplot(data=train, x=var_predictora, y=var_predecir, order=order)\n",
        "    elif plot_type == 'scatter':\n",
        "        sns.scatterplot(data=train, x=var_predictora, y=var_predecir)\n",
        "    plt.title(plot_title)\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "dn13OgZfwzJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANALISIS EXPLORATORIO**\n",
        "\n",
        "Analisis Exploratorio - Store"
      ],
      "metadata": {
        "id": "txh-BgpimMpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Buscamos entender los datos, qué info hay oculta en la data\n",
        "# Analizamos cada dataset\n",
        "print(store)\n",
        "#Observamos 1115 rows con datos adicionales de las stores\n",
        "print(store.shape)\n",
        "store.info() # Con data.info() podemos ver las variables categóricas\n",
        "store.isnull().sum() # hay 354 missings en CompetitionOpenSinceMonth, CompetitionOpenSinceYear, 3 en competition distance,\n",
        "#y 544 en Promo2SinceWeek, Promo2SinceYear, PromoInterval\n",
        "\n",
        "# Filtrar y mostrar las filas con NaN en la variable CompetitionDistance\n",
        "rows_with_na = store[store['CompetitionDistance'].isna()]\n",
        "print(rows_with_na) #tienen NAN en CompetitionDistance,CompetitionOpenSinceMonth,CompetitionOpenSinceYear,Promo2SinceWeek,Promo2SinceYear . La eliminaremos en featuring engeneering\n",
        "\n",
        "# Analizamos las cuatro variables categoricas\n",
        "columnas_cat = ['StoreType', 'Assortment', 'Promo2', 'PromoInterval']\n",
        "for col in columnas_cat:\n",
        "  print(f'{col} contiene: {store[col].nunique()} categorias diferentes')\n",
        "#StoreType contiene: 4 categorias diferentes (a,b,c,d)\n",
        "#Assortment contiene: 3 categorias diferentes\n",
        "#Promo2 contiene: 2 categorias diferentes (es binaria 0,1)\n",
        "#PromoInterval contiene: 3 categorias diferentes\n",
        "\n",
        "# StoreType: Analizamos la cantidad de stores dentro de cada tipo\n",
        "store_counts = store['StoreType'].value_counts()\n",
        "#a    602\n",
        "#d    348\n",
        "#c    148\n",
        "#b     17\n",
        "print(store_counts)\n",
        "graficar(store_counts.index, store_counts.values, 'bar', 'Sales by stores type')\n",
        "\n",
        "#Assortment: Analizamos la cantidad dentro de cada categoria\n",
        "store.Assortment.value_counts()\n",
        "#a    593\n",
        "#c    513\n",
        "#b      9\n",
        "\n",
        "# PromoInterval: Contamos unique values\n",
        "store.PromoInterval.value_counts()\n",
        "#Jan,Apr,Jul,Oct     335\n",
        "#Feb,May,Aug,Nov     130\n",
        "#Mar,Jun,Sept,Dec    106\n",
        "store.CompetitionOpenSinceYear.value_counts()\n",
        "conteos = store.CompetitionOpenSinceYear.value_counts()\n",
        "plt.bar(conteos.index, conteos.values) #se ve que hay un store que abrio en el 1900\n",
        "graficar(conteos.index, conteos.values, 'bar', 'Año de apertura de la competencia')\n",
        "\n",
        "store.describe() # se observa una gran dispersion en los datos de CompetitionDistance (std alta), los min y max se observan coherentes"
      ],
      "metadata": {
        "id": "QkxycFCFhiQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis Exploratorio - Train"
      ],
      "metadata": {
        "id": "ajqhP5wAhwFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.shape) # (1017209, 9) filas, cols\n",
        "print(train)\n",
        "train.info() # hay dos objects (string o cat), state holiday y day of week; el resto ints\n",
        "train.isnull().sum() #no hay datos faltantes!\n",
        "\n",
        "# Analizamos las unicas variables que no son de tipo int64\n",
        "columnas_cat = ['Date', 'StateHoliday']\n",
        "for col in columnas_cat:\n",
        "  print(f'Columna {col}: {train[col].nunique()} categorias diferentes') # 942 fechas diferentes, 5 categorias diferentes para StateHoliday,cuando deberian ser 4. Detectamos que es debido a un espacio en blanco\n",
        "print(\"Valores:\\n\", train['StateHoliday'].value_counts())\n",
        "#Columna Date: 942 categorias diferentes\n",
        "#Columna StateHoliday: 5 categorias diferentes\n",
        "\n",
        "print(train['StateHoliday'].unique()) #hay una mezcla de diferentes representaciones del valor cero, incluyendo '0' (como string) y 0 (como un entero).\n",
        "\n",
        "# Seleccionamos solo las columnas de tipo 'object'\n",
        "object_cols = train.select_dtypes(include=['object']).columns\n",
        "# Aplicamos strip() a todas las columnas de tipo 'object'\n",
        "train[object_cols] = train[object_cols].applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "print(\"Valores:\\n\", train['StateHoliday'].value_counts()) #Chequeamos que se soluciono el error en StateHoliday\n",
        "\n",
        "# El gráfico de recuento para la variable StateHoliday\n",
        "sns.countplot(x='StateHoliday', data=train)\n",
        "plt.xlabel('StateHoliday')\n",
        "plt.ylabel('Recuento')\n",
        "plt.title('Recuento de eventos por StateHoliday')\n",
        "plt.show()\n",
        "\n",
        "# El gráfico de recuento para la variable Date\n",
        "sns.countplot(x='Date', data=train)\n",
        "plt.xticks(rotation=50, fontsize=7)\n",
        "# Formatear las fechas en el eje x\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
        "plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Recuento')\n",
        "plt.title('Recuento de eventos por fecha')\n",
        "plt.show()\n",
        "# Vemos que hay muchos StateHoliday = 0. pocos a,b,c con mas a (public holiday).\n",
        "# Muchisimas fechas con un drop off en una seccion, podemos combinar por mes o algo para ver obs  mejor (a futuro creo que esta bueno reemplazarlo por cero por lo leido en la documentacion de quien subio el dataset en la competencia)\n",
        "\n",
        "train.describe()\n",
        "\n",
        "# Correlacion entre Sales vs Customers\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x='Customers', y='Sales', data=train)\n",
        "lin_fit = np.polyfit(train['Customers'], train['Sales'], 1)\n",
        "lin_func = np.poly1d(lin_fit)(train['Customers'])\n",
        "plt.plot(train['Customers'], lin_func, \"r--\", lw=1)\n",
        "plt.title(f\"Correlación entre Clientes y Ventas: {round(train['Customers'].corr(train['Sales'])*100, 2)}%\")\n",
        "# Cambiar etiquetas de los ejes\n",
        "plt.xlabel('Clientes')\n",
        "plt.ylabel('Ventas')\n",
        "plt.show()\n",
        "# Observamos la relacion linear entre customers y sales\n",
        "\n",
        "train.groupby('Open')['Sales'].sum() #confirma que no hay ventas cuando las tiendas estan cerradas\n",
        "train.groupby(['DayOfWeek', 'Promo']).mean()\n",
        "\n",
        "# Generamos un histograma para entender la distribucion de sales\n",
        "nbins = 75\n",
        "fig, ax = plt.subplots(figsize=(10, 7))\n",
        "sns.histplot(x='Sales', data=train, ax=ax, bins=nbins, kde=True)\n",
        "ax.set_xlabel('Ventas')\n",
        "ax.set_ylabel('Conteo')\n",
        "ax.set_title('Distribución de ventas')\n",
        "plt.show()\n",
        "# Sales 175k con 0, despues dist con mayoria de obs abajo de 10k gasto\n",
        "\n",
        "# Observamos como varian los valores de sales en las distintas stores\n",
        "graficar('Store', 'Sales', 'scatter', 'Sales variation in different stores')\n",
        "\n",
        "#Observamos columnas numericas\n",
        "cols_num = ['DayOfWeek','SchoolHoliday','Open']\n",
        "for col in cols_num:\n",
        "    graficar(col, 'Sales', 'bar', f'Ventas y {col}')\n",
        "\n",
        "  # Day of week, los domingos no se vende en la mayoria de los stores\n",
        "  # School Holiday, hay ventas igual\n",
        "  # Open: efectivamente no hay error, no hay ventas cuando cerrado\n",
        "\n",
        "#Observamos tiendas abiertas en funcion de los dias de la semana\n",
        "sns.countplot(x = 'DayOfWeek', hue = 'Open', data = train)\n",
        "plt.title('Conteo de tiendas abiertas por día')\n",
        "\n",
        "#Graficamos boxplots para ver outliers\n",
        "cols_num = ['Sales', 'Customers', 'Open', 'SchoolHoliday']\n",
        "\n",
        "for col in cols_num:\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.boxplot(x=col, data=train)\n",
        "    ax.set_title(col)\n",
        "    plt.show()\n",
        "\n",
        "# Comentarios:\n",
        "    # Hay un customer mas alla de los 7000\n",
        "    # Sales con muchos outliers pero obs con +40000 muy outlier\n",
        "    # Open y SchoolHoliday binarias"
      ],
      "metadata": {
        "id": "zic3zc5Whxwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis Exploratorio - Test"
      ],
      "metadata": {
        "id": "_jRvQJw3h6Z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(test.shape) #(41088, 8)\n",
        "print(test)\n",
        "test.info() #la variable open binaria esta en float y convertimos a integer\n",
        "test.isnull().sum() #estos nulos fueron error de quien cargo el dataset, se puede reemplazar por cero o eliminar el registro (son 11 valores, no es significativo eliminarlos con la base de datos que poseemos, pero decidimos reemplazar por 0)\n",
        "\n",
        "test['Open'] = test['Open'].astype(int)\n",
        "columnas_cat = ['Date', 'StateHoliday']\n",
        "for col in columnas_cat:\n",
        "  print(f'{col}: {test[col].nunique()} categorias diferentes')\n",
        "#Date: 48 categorias diferentes\n",
        "#StateHoliday: 2 categorias diferentes\n",
        "test.describe() #es racional lo descripto en lo devuelto en esta linea de codigo\n"
      ],
      "metadata": {
        "id": "L_nUoyfFh8yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FEATURING ENGENIEERING**\n",
        "\n",
        "Store + Train\n"
      ],
      "metadata": {
        "id": "Ne22_FVMkb6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Droppeamos costumers ya que la correlacion con sales es alta y\n",
        "train = train.drop(columns=['Customers'])\n",
        "\n",
        "#Corregimos este formato que habiamos descubierto en el EDA en Train y Test\n",
        "train['StateHoliday'] = train['StateHoliday'].replace(0, '0') # Convertimos the '0' string values a integers\n",
        "train.StateHoliday.value_counts() #se observa corregido\n",
        "test['Open'] = test['Open'].fillna(0) #error de carga de datos, el host de la competencia recomendaba reemplazar por cero\n",
        "print(test.isnull().sum().sum()) #se verifica y quedo bien el reemplazo\n",
        "\n",
        "# Transformamos Store para evitar que tenga datos faltantes y variables categoricas\n",
        "\n",
        "# Promo2SinceWeek, Promo2SinceYear, PromoInterval: Inputamos \"0\" a las variables numéricas y \"No promo\" a PromoInterval\n",
        "store['Promo2SinceWeek'].fillna(0, inplace=True)\n",
        "store['Promo2SinceYear'].fillna(0, inplace=True)\n",
        "store['PromoInterval'].fillna('No Promo', inplace=True)\n",
        "\n",
        "#Calulamos la media para CompetitionOpenSinceMonth y CompetitionOpenSinceYear ya que no queremos eliminarlas, y no tiene sentido reemplazar con 0\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "cols_to_impute = ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear']\n",
        "imputer.fit(store[cols_to_impute])\n",
        "store.loc[:, cols_to_impute] = imputer.transform(store[cols_to_impute])\n",
        "print(store.isnull().sum().sum()) # verificamos que no haya nulos\n",
        "\n",
        "#store[cols_to_impute] = imputer.transform(store[cols_to_impute])\n",
        "\n",
        "#Transformamos valores float a integer en train\n",
        "store['CompetitionDistance'] = store['CompetitionDistance'].astype(int)\n",
        "store['CompetitionOpenSinceMonth'] = store['CompetitionOpenSinceMonth'].astype(int)\n",
        "store['CompetitionOpenSinceYear'] = store['CompetitionOpenSinceYear'].astype(int)\n",
        "store['Promo2SinceWeek'] = store['Promo2SinceWeek'].astype(int)\n",
        "store['Promo2SinceYear'] = store['Promo2SinceYear'].astype(int)\n",
        "test['Open'] = test['Open'].astype(int)\n",
        "\n",
        "# Aplicamos One-Hot encoding a las variables categorigas.\n",
        "# Encoding Assorted - Al ser variables categoricas ordinales a = basic, b = extra, c = extended, las trasnformamos en 1,2,3.\n",
        "store['Assortment']=store['Assortment'].map({'a':1,'b':2,'c':3})\n",
        "print(store['Assortment'].value_counts())\n",
        "\n",
        "#Encoding Promo Interval\n",
        "store['PromoInterval']=store['PromoInterval'].map({'No Promo':0,'Jan,Apr,Jul,Oct':1,'Feb,May,Aug,Nov':2,'Mar,Jun,Sept,Dec':3})\n",
        "print(store['PromoInterval'].value_counts())\n",
        "\n",
        "#Encoding StoreType\n",
        "store['StoreType']=store['StoreType'].map({'a':1,'b':2,'c':3,'d':4})\n",
        "print(store['StoreType'].value_counts())\n",
        "\n",
        "# Hacer state holiday booleana en train y test\n",
        "train['StateHoliday'] = train['StateHoliday'].map({'0': 0, 'a': 1, 'b': 1, 'c': 1})\n",
        "print(train['StateHoliday'].value_counts())\n",
        "test['StateHoliday'] = test['StateHoliday'].map({'0': 0, 'a': 1, 'b': 1, 'c': 1})\n",
        "print(test['StateHoliday'].value_counts())\n",
        "\n",
        "#Verificamos que quedaron todas las variables corregidas y en correcto formato para poder hacer el merge\n",
        "store.info()\n",
        "train.info()\n",
        "test.info()\n",
        "store.isnull().sum()\n",
        "train.isnull().sum()\n",
        "test.isnull().sum()\n",
        "print(store)\n",
        "print(train)\n",
        "print(test)"
      ],
      "metadata": {
        "id": "VjLV3kbnuAKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f23e05-cca9-4d54-9999-03d417df631e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "1    593\n",
            "3    513\n",
            "2      9\n",
            "Name: Assortment, dtype: int64\n",
            "0    544\n",
            "1    335\n",
            "2    130\n",
            "3    106\n",
            "Name: PromoInterval, dtype: int64\n",
            "1    602\n",
            "4    348\n",
            "3    148\n",
            "2     17\n",
            "Name: StoreType, dtype: int64\n",
            "0    986159\n",
            "1     31050\n",
            "Name: StateHoliday, dtype: int64\n",
            "0    40908\n",
            "1      180\n",
            "Name: StateHoliday, dtype: int64\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1115 entries, 0 to 1114\n",
            "Data columns (total 10 columns):\n",
            " #   Column                     Non-Null Count  Dtype\n",
            "---  ------                     --------------  -----\n",
            " 0   Store                      1115 non-null   int64\n",
            " 1   StoreType                  1115 non-null   int64\n",
            " 2   Assortment                 1115 non-null   int64\n",
            " 3   CompetitionDistance        1115 non-null   int64\n",
            " 4   CompetitionOpenSinceMonth  1115 non-null   int64\n",
            " 5   CompetitionOpenSinceYear   1115 non-null   int64\n",
            " 6   Promo2                     1115 non-null   int64\n",
            " 7   Promo2SinceWeek            1115 non-null   int64\n",
            " 8   Promo2SinceYear            1115 non-null   int64\n",
            " 9   PromoInterval              1115 non-null   int64\n",
            "dtypes: int64(10)\n",
            "memory usage: 87.2 KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1017209 entries, 0 to 1017208\n",
            "Data columns (total 8 columns):\n",
            " #   Column         Non-Null Count    Dtype         \n",
            "---  ------         --------------    -----         \n",
            " 0   Store          1017209 non-null  int64         \n",
            " 1   DayOfWeek      1017209 non-null  int64         \n",
            " 2   Date           1017209 non-null  datetime64[ns]\n",
            " 3   Sales          1017209 non-null  int64         \n",
            " 4   Open           1017209 non-null  int64         \n",
            " 5   Promo          1017209 non-null  int64         \n",
            " 6   StateHoliday   1017209 non-null  int64         \n",
            " 7   SchoolHoliday  1017209 non-null  int64         \n",
            "dtypes: datetime64[ns](1), int64(7)\n",
            "memory usage: 62.1 MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 41088 entries, 0 to 41087\n",
            "Data columns (total 8 columns):\n",
            " #   Column         Non-Null Count  Dtype         \n",
            "---  ------         --------------  -----         \n",
            " 0   Id             41088 non-null  int64         \n",
            " 1   Store          41088 non-null  int64         \n",
            " 2   DayOfWeek      41088 non-null  int64         \n",
            " 3   Date           41088 non-null  datetime64[ns]\n",
            " 4   Open           41088 non-null  int64         \n",
            " 5   Promo          41088 non-null  int64         \n",
            " 6   StateHoliday   41088 non-null  int64         \n",
            " 7   SchoolHoliday  41088 non-null  int64         \n",
            "dtypes: datetime64[ns](1), int64(7)\n",
            "memory usage: 2.5 MB\n",
            "      Store  StoreType  Assortment  CompetitionDistance  CompetitionOpenSinceMonth  CompetitionOpenSinceYear  Promo2  Promo2SinceWeek  Promo2SinceYear  PromoInterval\n",
            "0         1          3           1                 1270                          9                      2008       0                0                0              0\n",
            "1         2          1           1                  570                         11                      2007       1               13             2010              1\n",
            "2         3          1           1                14130                         12                      2006       1               14             2011              1\n",
            "3         4          3           3                  620                          9                      2009       0                0                0              0\n",
            "4         5          1           1                29910                          4                      2015       0                0                0              0\n",
            "...     ...        ...         ...                  ...                        ...                       ...     ...              ...              ...            ...\n",
            "1110   1111          1           1                 1900                          6                      2014       1               31             2013              1\n",
            "1111   1112          3           3                 1880                          4                      2006       0                0                0              0\n",
            "1112   1113          1           3                 9260                          7                      2008       0                0                0              0\n",
            "1113   1114          1           3                  870                          7                      2008       0                0                0              0\n",
            "1114   1115          4           3                 5350                          7                      2008       1               22             2012              3\n",
            "\n",
            "[1115 rows x 10 columns]\n",
            "         Store  DayOfWeek       Date  Sales  Open  Promo  StateHoliday  SchoolHoliday\n",
            "0            1          5 2015-07-31   5263     1      1             0              1\n",
            "1            2          5 2015-07-31   6064     1      1             0              1\n",
            "2            3          5 2015-07-31   8314     1      1             0              1\n",
            "3            4          5 2015-07-31  13995     1      1             0              1\n",
            "4            5          5 2015-07-31   4822     1      1             0              1\n",
            "...        ...        ...        ...    ...   ...    ...           ...            ...\n",
            "1017204   1111          2 2013-01-01      0     0      0             1              1\n",
            "1017205   1112          2 2013-01-01      0     0      0             1              1\n",
            "1017206   1113          2 2013-01-01      0     0      0             1              1\n",
            "1017207   1114          2 2013-01-01      0     0      0             1              1\n",
            "1017208   1115          2 2013-01-01      0     0      0             1              1\n",
            "\n",
            "[1017209 rows x 8 columns]\n",
            "          Id  Store  DayOfWeek       Date  Open  Promo  StateHoliday  SchoolHoliday\n",
            "0          1      1          4 2015-09-17     1      1             0              0\n",
            "1          2      3          4 2015-09-17     1      1             0              0\n",
            "2          3      7          4 2015-09-17     1      1             0              0\n",
            "3          4      8          4 2015-09-17     1      1             0              0\n",
            "4          5      9          4 2015-09-17     1      1             0              0\n",
            "...      ...    ...        ...        ...   ...    ...           ...            ...\n",
            "41083  41084   1111          6 2015-08-01     1      0             0              0\n",
            "41084  41085   1112          6 2015-08-01     1      0             0              0\n",
            "41085  41086   1113          6 2015-08-01     1      0             0              0\n",
            "41086  41087   1114          6 2015-08-01     1      0             0              0\n",
            "41087  41088   1115          6 2015-08-01     1      0             0              1\n",
            "\n",
            "[41088 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Externa"
      ],
      "metadata": {
        "id": "h3HYkEnBceV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Le hacemos trasnformaciones a Weather para poder usarla\n",
        "\n",
        "lista_eventos = ['', 'Fog-Rain', 'Fog-Snow', 'Fog-Thunderstorm',\n",
        "              'Rain-Snow-Hail-Thunderstorm', 'Rain-Snow', 'Rain-Snow-Hail',\n",
        "              'Fog-Rain-Hail', 'Fog', 'Fog-Rain-Hail-Thunderstorm', 'Fog-Snow-Hail',\n",
        "              'Rain-Hail', 'Rain-Hail-Thunderstorm', 'Fog-Rain-Snow', 'Rain-Thunderstorm',\n",
        "              'Fog-Rain-Snow-Hail', 'Rain', 'Thunderstorm', 'Snow-Hail',\n",
        "              'Rain-Snow-Thunderstorm', 'Snow', 'Fog-Rain-Thunderstorm']\n",
        "lista_eventos_map = dict(zip(lista_eventos , range(len(lista_eventos))))\n",
        "#Confirmar el mapping\n",
        "[(k,v) for k,v in lista_eventos_map.items()][:3]\n",
        "\n",
        "def nombres_estados_a_abreviaciones(nombre_estado):\n",
        "    d = {}\n",
        "    d['BadenWürttemberg'] = 'BW'\n",
        "    d['Bayern'] = 'BY'\n",
        "    d['Berlin'] = 'BE'\n",
        "    d['Brandenburg'] = 'BB'  # no existe en store_state\n",
        "    d['Bremen'] = 'HB'  # usamos Niedersachsen en lugar de Bremen\n",
        "    d['Hamburg'] = 'HH'\n",
        "    d['Hessen'] = 'HE'\n",
        "    d['MecklenburgVorpommern'] = 'MV'  # no existe en store_state\n",
        "    d['Niedersachsen'] = 'HB,NI'  # usamos Niedersachsen en lugar de Bremen\n",
        "    d['NordrheinWestfalen'] = 'NW'\n",
        "    d['RheinlandPfalz'] = 'RP'\n",
        "    d['Saarland'] = 'SL'\n",
        "    d['Sachsen'] = 'SN'\n",
        "    d['SachsenAnhalt'] = 'ST'\n",
        "    d['SchleswigHolstein'] = 'SH'\n",
        "    d['Thüringen'] = 'TH'\n",
        "\n",
        "    return d[nombre_estado]\n",
        "\n",
        "lista_clima = []\n",
        "for estado_aleman in tiempo_de_los_estados_alemanes:\n",
        "    nombre_estado = os.path.splitext(os.path.basename(estado_aleman))[0]\n",
        "    codigo_estado = nombres_estados_a_abreviaciones(nombre_estado)\n",
        "    clima = pd.read_csv(estado_aleman, delimiter=\";\", parse_dates=['Date'])\n",
        "    clima['State'] = codigo_estado\n",
        "\n",
        "    for temp in ['Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC']:\n",
        "        clima[temp] = (clima[temp] - 10) / 30\n",
        "\n",
        "    for humi in ['Max_Humidity','Mean_Humidity', 'Min_Humidity']:\n",
        "        clima[humi] = (clima[humi] - 50) / 50\n",
        "\n",
        "    clima['Max_Wind_SpeedKm_h'] = clima['Max_Wind_SpeedKm_h'] / 50\n",
        "    clima['Mean_Wind_SpeedKm_h'] = clima['Mean_Wind_SpeedKm_h'] / 30\n",
        "    clima['CloudCover'].fillna(0,inplace=True)\n",
        "    clima['Events'] = clima['Events'].map(lista_eventos_map)\n",
        "    #El evento climático en blanco es el índice 0\n",
        "    clima['Events'].fillna(0,inplace=True)\n",
        "    clima = clima[['Date','State','Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC','Max_Humidity','Mean_Humidity', 'Min_Humidity',\\\n",
        "                      'Max_Wind_SpeedKm_h','Mean_Wind_SpeedKm_h','CloudCover','Events']]\n",
        "    lista_clima.append(clima)\n",
        "\n",
        "clima_total = pd.concat(lista_clima, ignore_index=True)\n",
        "\n",
        "print(clima_total.isnull().sum().sum())\n",
        "clima_total.head()"
      ],
      "metadata": {
        "id": "rbZ8vzD1Y8ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Set Final: train_merged, test_merged\n"
      ],
      "metadata": {
        "id": "54urmOI6clfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Combinamos Train y Store trasnformado\n",
        "print(store['Store'].duplicated().sum())\n",
        "store = store.drop_duplicates(subset='Store')\n",
        "train['Store'] = train['Store'].astype(int)\n",
        "test['Store'] = test['Store'].astype(int)\n",
        "\n",
        "train_merged = train.merge(store, how='left', on='Store', validate='many_to_one')\n",
        "print(train_merged.shape)\n",
        "train_merged.to_csv('train_merged.csv', index=False)  # Guardar en un archivo CSV\n",
        "\n",
        "# Combinamos test y store\n",
        "test_merged = test.merge(store, how='left', on='Store')\n",
        "print(test_merged.shape)\n",
        "test_merged.to_csv('test_merged.csv', index=False)  # Guardar en un archivo CSV\n",
        "\n",
        "# Leer los datos combinados desde los archivos CSV\n",
        "train_merged = pd.read_csv('train_merged.csv', low_memory=False)\n",
        "test_merged = pd.read_csv('test_merged.csv', low_memory=False)\n",
        "\n",
        "#Se chequea que quedo todo correcto post mergeada\n",
        "train_merged.head()\n",
        "test_merged.head()\n",
        "train_merged.info()\n",
        "test_merged.info()\n",
        "train_merged.isnull().sum()\n",
        "test_merged.isnull().sum()\n",
        "\n",
        "### Creamos nuevas variables y transformamos\n",
        "\n",
        "#Transformaciones para Date\n",
        "# Convertimos Date a datetime type\n",
        "train_merged['Date'] = pd.to_datetime(train_merged['Date'])\n",
        "test_merged['Date'] = pd.to_datetime(test_merged['Date'])\n",
        "\n",
        "# Desagregamos date y extraemos info en nuevas variables\n",
        "train_merged['Year'] = train_merged['Date'].dt.year\n",
        "train_merged['Month'] = train_merged['Date'].dt.month\n",
        "train_merged['Day'] = train_merged['Date'].dt.day\n",
        "train_merged['Weekday'] = train_merged['Date'].dt.weekday\n",
        "train_merged['Quarter'] = train_merged['Date'].dt.quarter\n",
        "\n",
        "test_merged['Year'] = test_merged['Date'].dt.year\n",
        "test_merged['Month'] = test_merged['Date'].dt.month\n",
        "test_merged['Day'] = test_merged['Date'].dt.day\n",
        "test_merged['Weekday'] = test_merged['Date'].dt.weekday\n",
        "test_merged['Quarter'] = test_merged['Date'].dt.quarter\n",
        "\n",
        "test_merged.info()\n",
        "train_merged.info()\n",
        "\n",
        "# Vemos los dfs modificados\n",
        "print(train_merged.head())\n",
        "print(test_merged.head())\n",
        "\n",
        "# Verificamos las columnas en los dfs\n",
        "print(train_merged.columns)  # Display all columns in train_merged df\n",
        "print(test_merged.columns)\n",
        "\n",
        "### Combinamos con Data Externa\n",
        "\n",
        "#State\n",
        "train_merged = train_merged.merge(store_state,how='left',on='Store')\n",
        "print(train_merged.shape)\n",
        "print(\"train missing value \",train_merged.isnull().sum().sum())\n",
        "train_merged.head()\n",
        "\n",
        "test_merged = test_merged.merge(store_state,how='left',on='Store')\n",
        "print(test_merged.shape)\n",
        "print(\"test missing value \",test_merged.isnull().sum().sum())\n",
        "test_merged.head()\n",
        "\n",
        "# Weather\n",
        "train_merged = train_merged.merge(clima_total,how='left',left_on=['State','Date'],right_on=['State','Date'])\n",
        "print(train_merged.shape)\n",
        "print(\"train missing value \",train_merged.isnull().sum().sum())\n",
        "train_merged.head()\n",
        "\n",
        "test_merged = test_merged.merge(clima_total,how='left',left_on=['State','Date'],right_on=['State','Date'])\n",
        "print(test_merged.shape)\n",
        "print(\"test missing value \",test_merged.isnull().sum().sum())\n",
        "test_merged.head()\n",
        "\n",
        "# Hacemos One - Hot Encoding con State\n",
        "train_merged = pd.get_dummies(train_merged, columns=['State'])\n",
        "test_merged = pd.get_dummies(test_merged, columns=['State'])\n",
        "\n",
        "# Muestra las primeras filas del conjunto de datos codificado\n",
        "print(train_merged.head())\n",
        "print(test_merged.head())"
      ],
      "metadata": {
        "id": "I116w984hYHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9df210a-1fbd-4b52-a0d6-23231038761d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "(1017209, 17)\n",
            "(41088, 17)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1017209 entries, 0 to 1017208\n",
            "Data columns (total 17 columns):\n",
            " #   Column                     Non-Null Count    Dtype \n",
            "---  ------                     --------------    ----- \n",
            " 0   Store                      1017209 non-null  int64 \n",
            " 1   DayOfWeek                  1017209 non-null  int64 \n",
            " 2   Date                       1017209 non-null  object\n",
            " 3   Sales                      1017209 non-null  int64 \n",
            " 4   Open                       1017209 non-null  int64 \n",
            " 5   Promo                      1017209 non-null  int64 \n",
            " 6   StateHoliday               1017209 non-null  int64 \n",
            " 7   SchoolHoliday              1017209 non-null  int64 \n",
            " 8   StoreType                  1017209 non-null  int64 \n",
            " 9   Assortment                 1017209 non-null  int64 \n",
            " 10  CompetitionDistance        1017209 non-null  int64 \n",
            " 11  CompetitionOpenSinceMonth  1017209 non-null  int64 \n",
            " 12  CompetitionOpenSinceYear   1017209 non-null  int64 \n",
            " 13  Promo2                     1017209 non-null  int64 \n",
            " 14  Promo2SinceWeek            1017209 non-null  int64 \n",
            " 15  Promo2SinceYear            1017209 non-null  int64 \n",
            " 16  PromoInterval              1017209 non-null  int64 \n",
            "dtypes: int64(16), object(1)\n",
            "memory usage: 131.9+ MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 41088 entries, 0 to 41087\n",
            "Data columns (total 17 columns):\n",
            " #   Column                     Non-Null Count  Dtype \n",
            "---  ------                     --------------  ----- \n",
            " 0   Id                         41088 non-null  int64 \n",
            " 1   Store                      41088 non-null  int64 \n",
            " 2   DayOfWeek                  41088 non-null  int64 \n",
            " 3   Date                       41088 non-null  object\n",
            " 4   Open                       41088 non-null  int64 \n",
            " 5   Promo                      41088 non-null  int64 \n",
            " 6   StateHoliday               41088 non-null  int64 \n",
            " 7   SchoolHoliday              41088 non-null  int64 \n",
            " 8   StoreType                  41088 non-null  int64 \n",
            " 9   Assortment                 41088 non-null  int64 \n",
            " 10  CompetitionDistance        41088 non-null  int64 \n",
            " 11  CompetitionOpenSinceMonth  41088 non-null  int64 \n",
            " 12  CompetitionOpenSinceYear   41088 non-null  int64 \n",
            " 13  Promo2                     41088 non-null  int64 \n",
            " 14  Promo2SinceWeek            41088 non-null  int64 \n",
            " 15  Promo2SinceYear            41088 non-null  int64 \n",
            " 16  PromoInterval              41088 non-null  int64 \n",
            "dtypes: int64(16), object(1)\n",
            "memory usage: 5.3+ MB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Id                           0\n",
              "Store                        0\n",
              "DayOfWeek                    0\n",
              "Date                         0\n",
              "Open                         0\n",
              "Promo                        0\n",
              "StateHoliday                 0\n",
              "SchoolHoliday                0\n",
              "StoreType                    0\n",
              "Assortment                   0\n",
              "CompetitionDistance          0\n",
              "CompetitionOpenSinceMonth    0\n",
              "CompetitionOpenSinceYear     0\n",
              "Promo2                       0\n",
              "Promo2SinceWeek              0\n",
              "Promo2SinceYear              0\n",
              "PromoInterval                0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matriz de correlacion variables\n"
      ],
      "metadata": {
        "id": "z_Yls5jYkoDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generamos una matriz de correlación usando todas las features\n",
        "var_correlacion = train_merged.corr().abs()\n",
        "\n",
        "# Graficamos un heat map\n",
        "fig, axes = plt.subplots(figsize=(12, 12))\n",
        "sns.heatmap(var_correlacion, annot = True, fmt='.2f', annot_kws={'size': 10},  vmax=.8, square=True, cmap='Blues');"
      ],
      "metadata": {
        "id": "Fv-rehVXkqkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MODELOS**"
      ],
      "metadata": {
        "id": "x0tO2X7YkObI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cosideramos una serie de modelos, entre ellos Xgboost, Random Forest, LightGBM.\n",
        "- Para la creacion del conjunto de validation utilizamos en primer lugar un reordenamiento cronologico cortando en 80/20 el set de train.\n",
        "- Luego definimos y utilizamos TimeSeriesSplit.\n",
        "- En funcion de estas tecnicas de CV optimizamos los hiperparametros de cada modelo.  "
      ],
      "metadata": {
        "id": "45B_5S58jmEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hold-Out Set"
      ],
      "metadata": {
        "id": "5TwW6WS_dpw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tomamos un hold out set (80/20) para hcer cross-validation\n",
        "# Ordenamos el conjunto de datos por año y mes en orden ascendente para considerar la temporalidad de los datos.\n",
        "sorted_data = train_merged.sort_values(['Year', 'Month'])\n",
        "\n",
        "# Calcular el índice de corte\n",
        "cut_index = int(np.floor(len(sorted_data) * 0.8))\n",
        "\n",
        "# Dividir el conjunto de datos en entrenamiento y validación\n",
        "train_data = sorted_data.iloc[:cut_index]\n",
        "valid_data = sorted_data.iloc[cut_index:]\n",
        "\n",
        "# Obtener los índices correspondientes a los conjuntos de entrenamiento y validación\n",
        "train_idx = train_data.index.values\n",
        "valid_idx = valid_data.index.values\n",
        "\n",
        "splits = (list(train_idx), list(valid_idx))\n",
        "\n",
        "# Verificamos la cantidad de filas de los data sets\n",
        "print(train_merged.shape)\n",
        "print(train_data.shape)\n",
        "print(valid_data.shape)"
      ],
      "metadata": {
        "id": "JbDfzohQsZ3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TimeSeriesSplit"
      ],
      "metadata": {
        "id": "6GXbwGWvdfdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Probamos TimeSeriesFolds para generar folds de train y validations sets que consideren la temporalidad del data frame.\n",
        "# Analizamos que los data sets tscv generados por TimeSeriesSplit sean correctos\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "x = train_merged.drop(columns=['Sales', 'Date']).values\n",
        "y = np.log(train_merged['Sales'] + 1).values\n",
        "\n",
        "# Inicializamos listas para guardar los resultados\n",
        "x_train_list = []\n",
        "x_val_list = []\n",
        "y_train_list = []\n",
        "y_val_list = []\n",
        "\n",
        "train_index_list = []\n",
        "val_index_list = []\n",
        "\n",
        "for train_index, val_index in tscv.split(x):\n",
        "    train_index_list.append(train_index)\n",
        "    val_index_list.append(val_index)\n",
        "\n",
        "train_index_list\n",
        "val_index_list"
      ],
      "metadata": {
        "id": "oRUGvUibdfGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XG BOOST**\n",
        "\n",
        "XG Boost + Hold Out Set"
      ],
      "metadata": {
        "id": "ORjpHbuosrEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.utils import parallel_backend\n",
        "\n",
        "x_train = train_data.drop(columns=['Sales','Date'])\n",
        "y_train = log(train_data['Sales']+1)\n",
        "x_test = valid_data.drop(columns=['Sales', 'Date'])\n",
        "y_test = log(valid_data['Sales']+1)\n",
        "\n",
        "# Usamos random search para iterar entre los parámetros de XGBoost\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 200],\n",
        "    'colsample_bytree': [0.7, 0.8],\n",
        "    'max_depth': [15,20,25],\n",
        "    'reg_alpha': [1.1, 1.2, 1.3],\n",
        "    'reg_lambda': [1.1, 1.2, 1.3],\n",
        "    'subsample': [0.7, 0.8, 0.9]\n",
        "}\n",
        "\n",
        "# RMSPE\n",
        "def rmspe(y_true, y_pred):\n",
        "    non_zero_mask = y_true != 0\n",
        "    y_true_non_zero = y_true[non_zero_mask]\n",
        "    y_pred_non_zero = y_pred[non_zero_mask]\n",
        "    percentage_error = np.abs((y_true_non_zero - y_pred_non_zero) / y_true_non_zero)\n",
        "    rmspe = np.sqrt(np.mean(np.square(percentage_error)))\n",
        "    return rmspe\n",
        "\n",
        "rmspe_scorer = make_scorer(rmspe, greater_is_better=False)\n",
        "\n",
        "# Entrenamos el modelo\n",
        "xgb_reg = xgb.XGBRegressor(seed=42)\n",
        "random_search = RandomizedSearchCV(\n",
        "    xgb_reg,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=5,  # Numero de iteraciones\n",
        "    scoring=rmspe_scorer,\n",
        "    n_jobs=-1,  # Corre jobs en paralelo\n",
        "    random_state=42,\n",
        "    cv=5,\n",
        "    verbose=3  # Verbosity\n",
        ")\n",
        "random_search.fit(x_train, y_train)\n",
        "\n",
        "# Obtenemos los parámetros que mejor performan\n",
        "print(\"Mejores Parámetros XGBoost: \", random_search.best_params_)\n",
        "\n",
        "# Predecimos en test\n",
        "y_pred = random_search.predict(x_test)\n",
        "\n",
        "# Calculamos RMSPE\n",
        "print(\"RMSPE = \", rmspe(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "RUe77SUelc-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3bfbd51-5862-4fa4-9a86-11af87df4de4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XG Boost + TimeSeriesSplit"
      ],
      "metadata": {
        "id": "PKfHhm4O7xAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "import lightgbm as lgb\n",
        "\n",
        "train_merged = train_merged.sort_values(['Year', 'Month'])\n",
        "x = train_merged.drop(columns=['Sales', 'Date']).values\n",
        "y = np.log(train_merged['Sales'] + 1).values\n",
        "\n",
        "def rmspe(y_true, y_pred):\n",
        "    non_zero_mask = y_true != 0\n",
        "    y_true_non_zero = y_true[non_zero_mask]\n",
        "    y_pred_non_zero = y_pred[non_zero_mask]\n",
        "    percentage_error = np.abs((y_true_non_zero - y_pred_non_zero) / y_true_non_zero)\n",
        "    rmspe = np.sqrt(np.mean(np.square(percentage_error)))\n",
        "    return rmspe\n",
        "\n",
        "rmspe_scorer = make_scorer(rmspe, greater_is_better=False)\n",
        "\n",
        "# Usamos TimeSeriesSplit para crear 5 folds\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Elegimos parámetros posibles\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 200],\n",
        "    'colsample_bytree': [0.7, 0.8],\n",
        "    'max_depth': [15, 20, 25],\n",
        "    'reg_alpha': [1.1, 1.2, 1.3],\n",
        "    'reg_lambda': [1.1, 1.2, 1.3],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'num_leaves': [31, 63, 127, 255],\n",
        "\n",
        "}\n",
        "\n",
        "rmspe_values = []\n",
        "best_params = {}\n",
        "best_rmspe = 1.0\n",
        "\n",
        "for train_index, val_index in tscv.split(x):\n",
        "    x_train, x_val = x[train_index], x[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "    lgb_reg = lgb.LGBMRegressor(seed=42)\n",
        "    random_search = RandomizedSearchCV(\n",
        "        lgb_reg,\n",
        "        param_distributions=param_grid,\n",
        "        n_iter=5,\n",
        "        scoring=rmspe_scorer,\n",
        "        n_jobs=-1,\n",
        "        random_state=42,\n",
        "        cv=3, #para no ovverfittear\n",
        "        verbose=3\n",
        "    )\n",
        "\n",
        "    random_search.fit(x_train, y_train)\n",
        "\n",
        "    y_pred = random_search.predict(x_val)\n",
        "\n",
        "    rmspe_val = rmspe(y_val, y_pred)\n",
        "    rmspe_values.append(rmspe_val)\n",
        "\n",
        "    if rmspe_val < best_rmspe:\n",
        "        best_params = random_search.best_params_\n",
        "        best_rmspe = rmspe_val\n",
        "\n",
        "avg_rmspe = np.mean(rmspe_values)\n",
        "print(\"Average RMSPE =\", avg_rmspe)\n",
        "print(\"Best parameters:\", best_params)\n",
        "\n",
        "# Feature importance cuando corremos con time series\n",
        "lgb_reg = lgb.LGBMRegressor(**best_params)\n",
        "lgb_reg.fit(x, y)\n",
        "\n",
        "# Obtenemos las variables + importantes\n",
        "importances = lgb_reg.feature_importances_\n",
        "importances_df = pd.DataFrame({'feature': train_merged.drop(columns=['Sales', 'Date']).columns, 'importance': importances})\n",
        "importances_df = importances_df.sort_values('importance', ascending=False)\n",
        "print(\"Important variables:\", importances_df)\n",
        "\n",
        "# Graficamos\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Feature importances\")\n",
        "plt.barh(importances_df['feature'], importances_df['importance'], align='center')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-eAFMik-71i3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8b309c7-f6a1-4fd0-cbd5-02dc7aa4cc31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RANDOM FOREST**\n",
        "\n",
        "Random Forest + TimeSeriesSplit"
      ],
      "metadata": {
        "id": "eeXvBOIdoSgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "train_merged = train_merged.sort_values(['Year', 'Month'])\n",
        "x = train_merged.drop(columns=['Sales', 'Date']).values\n",
        "y = np.log(train_merged['Sales'] + 1).values\n",
        "\n",
        "def rmspe(y_true, y_pred):\n",
        "    non_zero_mask = y_true != 0\n",
        "    y_true_non_zero = y_true[non_zero_mask]\n",
        "    y_pred_non_zero = y_pred[non_zero_mask]\n",
        "    percentage_error = np.abs((y_true_non_zero - y_pred_non_zero) / y_true_non_zero)\n",
        "    rmspe = np.sqrt(np.mean(np.square(percentage_error)))\n",
        "    return rmspe\n",
        "\n",
        "rmspe_scorer = make_scorer(rmspe, greater_is_better=False)\n",
        "\n",
        "# Use TimeSeriesSplit to create 5 folds\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [250, 300, 350],\n",
        "    'max_depth': [5, 10, 15],\n",
        "    'min_samples_split': [20, 30, 40],\n",
        "    'min_samples_leaf': [5, 10, 15],\n",
        "    'max_features': [5, 10, 15]\n",
        "}\n",
        "\n",
        "rf_reg = RandomForestRegressor(random_state=42)\n",
        "random_search = RandomizedSearchCV(\n",
        "        rf_reg,\n",
        "        param_distributions=param_grid,\n",
        "        n_iter=5,\n",
        "        scoring=rmspe_scorer,\n",
        "        n_jobs=-1,\n",
        "        cv=tscv,\n",
        "        verbose=3,\n",
        "        random_state=42\n",
        ")\n",
        "\n",
        "random_search.fit(x,y)\n",
        "\n",
        "# Best Params\n",
        "print(\"Best Parameters: \", random_search.best_params_)\n",
        "\n",
        "# RMSPE score\n",
        "print(\"Best Score (RMSPE): \", -random_search.best_score_)\n",
        "\n",
        "# Feature importance\n",
        "best_params = random_search.best_params_\n",
        "rf_reg = RandomForestRegressor(**best_params, random_state=42)\n",
        "rf_reg.fit(x,y)\n",
        "\n",
        "importances = rf_reg.feature_importances_\n",
        "importances_df = pd.DataFrame({'feature': train_merged.drop(columns=['Sales', 'Date']).columns, 'importance': importances})\n",
        "importances_df = importances_df.sort_values('importance', ascending=False)\n",
        "print(\"Important variables:\", importances_df)\n",
        "\n",
        "# Plot the feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Feature importances\")\n",
        "plt.barh(importances_df['feature'], importances_df['importance'], align='center')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QZGQRSGkL1Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest + TimeSeriesSplit + Predicciones OOB"
      ],
      "metadata": {
        "id": "RR6dkbxZuIMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "train_merged = train_merged[train_merged['Open'] != 0]\n",
        "test_merged = test_merged[test_merged['Open'] != 0]\n",
        "\n",
        "# Reordering train set\n",
        "sorted_data = train_merged.sort_values(['Year', 'Month'])\n",
        "\n",
        "x = sorted_data.drop(columns=['Sales', 'Date'])\n",
        "y = np.log(sorted_data['Sales'] + 1)\n",
        "\n",
        "# RMSPE scorer\n",
        "def rmspe(y_true, y_pred):\n",
        "    non_zero_mask = y_true != 0\n",
        "    y_true_non_zero = y_true[non_zero_mask]\n",
        "    y_pred_non_zero = y_pred[non_zero_mask]\n",
        "    percentage_error = np.abs((y_true_non_zero - y_pred_non_zero) / y_true_non_zero)\n",
        "    rmspe = np.sqrt(np.mean(np.square(percentage_error)))\n",
        "    return rmspe\n",
        "\n",
        "rmspe_scorer = make_scorer(rmspe, greater_is_better=False)\n",
        "\n",
        "# Parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [250, 300, 350],\n",
        "    'max_depth': [5, 10, 15],\n",
        "    'min_samples_split': [25, 30, 50],\n",
        "    'min_samples_leaf': [5, 10, 15],\n",
        "    'max_features': [5, 10, 15, 20] # sume un 20 en max features\n",
        "}\n",
        "\n",
        "# Random Forest model\n",
        "rf_reg = RandomForestRegressor(oob_score=True, random_state=42)\n",
        "\n",
        "# TimeSeriesSplit\n",
        "n_splits = 5\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "# Randomized search\n",
        "random_search = RandomizedSearchCV(rf_reg,\n",
        "                                   param_distributions=param_grid,\n",
        "                                   n_iter=5,\n",
        "                                   scoring=rmspe_scorer,\n",
        "                                   n_jobs=-1,\n",
        "                                   cv=tscv,\n",
        "                                   verbose=3,\n",
        "                                   random_state=42)\n",
        "\n",
        "random_search.fit(x, y)\n",
        "\n",
        "# Mejores Parametros\n",
        "print(\"Best RF parameters: \", random_search.best_params_)\n",
        "\n",
        "# OOB Score\n",
        "print(f\"Out-of-bag score: {random_search.best_score_:.3f}\")\n"
      ],
      "metadata": {
        "id": "eq5vEJ0MNYmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest + Feature Importance + Modelo Final"
      ],
      "metadata": {
        "id": "Mp8cCutufacz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Entrenamos un modelo con las variables más importantes y un número de iteraciones grandes.\n",
        "#Usamos como data set = train + validation\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "x = train_merged.drop(columns=['Sales'])\n",
        "y = np.log(train_merged['Sales'] + 1)\n",
        "\n",
        "#Nos quedmaos únicamente con las features mas impoprtantes\n",
        "features_to_keep = [\"Open\", \"DayOfWeek\", \"Weekday\", \"StateHoliday\", \"Promo\",\n",
        "            \"CompetitionDistance\", \"Store\" , \"Day\", \"CompetitionOpenSinceYear\", \"CompetitionOpenSinceMonth\", \"Month\", \"StoreType\", \"Promo2SinceYear\",\n",
        "           \"Assortment\", \"Promo2SinceWeek\", \"State_NW\", \"PromoInterval\" ]\n",
        "\n",
        "x_important = x[features_to_keep]\n",
        "\n",
        "\n",
        "rf_reg_important = RandomForestRegressor(n_estimators=300,\n",
        "                                      min_samples_split=5,\n",
        "                                      min_samples_leaf=4,\n",
        "                                      max_features=24,\n",
        "                                      max_depth=25,\n",
        "                                      random_state=42,\n",
        "                                      verbose=3)\n",
        "rf_reg_important.fit(x_important, y)\n",
        "\n",
        "# Predecimos ocn el train set\n",
        "x_test = test_merged[features_to_keep]\n",
        "y_pred = rf_reg_important.predict(x_test)\n",
        "\n",
        "# Aplicar función exponencial inversa a las predicciones\n",
        "y_pred_real = np.exp(y_pred) - 1\n",
        "\n",
        "#generamos el sample submission\n",
        "sample_submission = pd.DataFrame({'Id': test['Id'], 'Sales': y_pred_real})\n",
        "\n",
        "# Export the DataFrame to a .csv file\n",
        "sample_submission.to_csv('sample_submission.csv', index=False)\n"
      ],
      "metadata": {
        "id": "0neEirP-fUO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LIGHTG BM**\n",
        "\n",
        "Light BM + TimeSeriesSplit"
      ],
      "metadata": {
        "id": "ZK6Lv0rMe-uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "import lightgbm as lgb\n",
        "\n",
        "train_marged = train_merged.sort_values(['Year', 'Month'])\n",
        "\n",
        "def rmspe(y_true, y_pred):\n",
        "    non_zero_mask = y_true != 0\n",
        "    y_true_non_zero = y_true[non_zero_mask]\n",
        "    y_pred_non_zero = y_pred[non_zero_mask]\n",
        "    percentage_error = np.abs((y_true_non_zero - y_pred_non_zero) / y_true_non_zero)\n",
        "    rmspe = np.sqrt(np.mean(np.square(percentage_error)))\n",
        "    return rmspe\n",
        "\n",
        "# Create rmspe scorer\n",
        "rmspe_scorer = make_scorer(rmspe, greater_is_better=False)\n",
        "\n",
        "# Utilize the TimeSeriesSplit module from sklearn\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Prepare the data\n",
        "x = train_merged.drop(columns=['Sales', 'Date']).values\n",
        "y = np.log(train_merged['Sales'] + 1).values\n",
        "\n",
        "# Perform cross-validation and parameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 300],\n",
        "    'colsample_bytree': [0.7, 0.8],\n",
        "    'max_depth': [10, 20, 25],\n",
        "    'reg_alpha': [1.1, 1.2, 1.3],\n",
        "    'reg_lambda': [1.1, 1.2, 1.3],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'num_leaves': [31, 63, 127, 255],\n",
        "\n",
        "}\n",
        "\n",
        "rmspe_values = []\n",
        "best_params = {}\n",
        "best_rmspe = 1.0\n",
        "\n",
        "for train_index, val_index in tscv.split(x):\n",
        "    x_train, x_val = x[train_index], x[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "    lgb_reg = lgb.LGBMRegressor(seed=42)\n",
        "    random_search = RandomizedSearchCV(\n",
        "        lgb_reg,\n",
        "        param_distributions=param_grid,\n",
        "        n_iter=5,\n",
        "        scoring=rmspe_scorer,\n",
        "        n_jobs=-1,\n",
        "        random_state=42,\n",
        "        cv=3, #para no overfittear\n",
        "        verbose=3\n",
        "    )\n",
        "\n",
        "    random_search.fit(x_train, y_train)\n",
        "\n",
        "    y_pred = random_search.predict(x_val)\n",
        "\n",
        "    rmspe_val = rmspe(y_val, y_pred)\n",
        "    rmspe_values.append(rmspe_val)\n",
        "\n",
        "    if rmspe_val < best_rmspe:\n",
        "        best_params = random_search.best_params_\n",
        "        best_rmspe = rmspe_val\n",
        "\n",
        "avg_rmspe = np.mean(rmspe_values)\n",
        "print(\"Average RMSPE =\", avg_rmspe)\n",
        "print(\"Best parameters:\", best_params)\n",
        "\n",
        "# Feature importance cuando corremos con time series\n",
        "lgb_reg = lgb.LGBMRegressor(**best_params)\n",
        "lgb_reg.fit(x, y)\n",
        "\n",
        "# Obtain the most important variables\n",
        "importances = lgb_reg.feature_importances_\n",
        "importances_df = pd.DataFrame({'feature': train_merged.drop(columns=['Sales', 'Date']).columns, 'importance': importances})\n",
        "importances_df = importances_df.sort_values('importance', ascending=False)\n",
        "print(\"Important variables:\", importances_df)\n",
        "\n",
        "# Plot the feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Feature importances\")\n",
        "plt.barh(importances_df['feature'], importances_df['importance'], align='center')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p_hfp-TTXDq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LightBM + Hold Out Set"
      ],
      "metadata": {
        "id": "AXXcghfit2JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lightgbm\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "#Definimos rmspe\n",
        "def rmspe(y_true, y_pred):\n",
        "    non_zero_mask = y_true != 0\n",
        "    y_true_non_zero = y_true[non_zero_mask]\n",
        "    y_pred_non_zero = y_pred[non_zero_mask]\n",
        "    percentage_error = np.abs((y_true_non_zero - y_pred_non_zero) / y_true_non_zero)\n",
        "    rmspe = np.sqrt(np.mean(np.square(percentage_error)))\n",
        "    return rmspe\n",
        "\n",
        "rmspe_scorer = make_scorer(rmspe, greater_is_better=False)\n",
        "\n",
        "# Definimos las limitaciones de los parametros\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 200],\n",
        "    'max_depth': [15,20,25],\n",
        "    'num_leaves': [20, 30, 40],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.7, 0.8, 0.9]\n",
        "}\n",
        "\n",
        "#Entrenamos al modelo\n",
        "lgbm_reg = LGBMRegressor(seed=42)\n",
        "random_search = RandomizedSearchCV(lgbm_reg,\n",
        "                                   param_distributions=param_grid,\n",
        "                                   n_iter=5,\n",
        "                                   scoring=rmspe_scorer,\n",
        "                                   n_jobs=-1,\n",
        "                                   cv=5,\n",
        "                                   verbose=3,\n",
        "                                   random_state=42)\n",
        "random_search.fit(x_train, y_train)\n",
        "\n",
        "# Obtenemos los mejores parámetros\n",
        "print(\"Mejores Parametros Lgbm: \",random_search.best_params_)\n",
        "\n",
        "# Predecimos en test\n",
        "y_pred = random_search.predict(x_test)\n",
        "\n",
        "# Calculamos RMSPE\n",
        "print(\"RMSPE = \", rmspe(y_test, y_pred))"
      ],
      "metadata": {
        "id": "WkcX5mEGqO61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Light BM + Feature Importance + Modelo Final\n",
        "\n"
      ],
      "metadata": {
        "id": "6vWDMPpmfmkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Entrenamos un modelo con las variables más importantes y un número de iteraciones grandes.\n",
        "#Usamos como data set = train + validation\n",
        "\n",
        "x = train_merged[[\"Store\" , \"DayOfWeek\",\"Open\", \"StoreType\" , \"Day\", \"Promo\", \"Month\", \"Assortment\", \"State_NW\",\n",
        "           \"PromoInterval\", \"State_SH\", \"Max_TemperatureC\", \"Weekday\"]]\n",
        "y = np.log(train_merged['Sales'] + 1)\n",
        "\n",
        "lgb_reg_important = lgb.LGBMRegressor(n_estimators=5000,\n",
        "                                      reg_lambda=1.1,\n",
        "                                      reg_alpha=1.3,\n",
        "                                      num_leaves=32,\n",
        "                                      max_depth=10,\n",
        "                                      colsample_bytree=0.7,\n",
        "                                      subsample=0.7,\n",
        "                                      random_state=42,\n",
        "                                      verbose=3)\n",
        "lgb_reg_important.fit(x, y)\n",
        "# Predecimos con el train set\n",
        "\n",
        "x_test = test_merged[[\"Store\" , \"DayOfWeek\",\"Open\", \"StoreType\" , \"Day\", \"Promo\", \"Month\", \"Assortment\", \"State_NW\",\n",
        "           \"PromoInterval\", \"State_SH\", \"Max_TemperatureC\", \"Weekday\"]]\n",
        "y_pred = lgb_reg_important.predict(x_test)\n",
        "\n",
        "# Aplicar función exponencial inversa a las predicciones\n",
        "y_pred_real = np.exp(y_pred) - 1\n",
        "\n",
        "# Generar el DataFrame para la muestra de envío\n",
        "sample_submission = pd.DataFrame({'Id': test['Id'], 'Sales': y_pred_real})\n",
        "\n",
        "# Exportar el DataFrame a un archivo CSV\n",
        "sample_submission.to_csv('sample_submission3.csv', index=False)\n"
      ],
      "metadata": {
        "id": "gk7m7UORfmWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INTERPRETACION MODELOS**"
      ],
      "metadata": {
        "id": "1Ax2XKt1ty_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Clustering Jerarquico - Dendograma\n",
        "\n",
        "from scipy.cluster import hierarchy\n",
        "from scipy.spatial import distance\n",
        "\n",
        "# Calculate Spearman correlation (rank correlation)\n",
        "corr_matrix = pd.DataFrame(x_important).corr(method='spearman')\n",
        "#fig, axes = plt.subplots(figsize=(20, 20))\n",
        "#sns.heatmap(corr_matrix, annot = True, fmt='.2f', annot_kws={'size': 10},  vmax=.8, square=True, cmap='Blues')\n",
        "\n",
        "# Convert correlation values to distances\n",
        "dist_matrix = 1 - np.abs(corr_matrix)\n",
        "\n",
        "# Create a hierarchical clustering plot\n",
        "linkage = distance.squareform(dist_matrix)\n",
        "dendrogram = hierarchy.dendrogram(hierarchy.linkage(linkage, method='average'))\n",
        "\n",
        "### ICE Plot\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=[10,8])\n",
        "\n",
        "common_params = {\"grid_resolution\": 20, \"random_state\": 42}\n",
        "features_info = {\"features\": [\"Year\"], \"kind\": \"both\", \"centered\": True}\n",
        "\n",
        "display = PartialDependenceDisplay.from_estimator(\n",
        "    rf_reg,\n",
        "    x_train,\n",
        "    **features_info,\n",
        "    ax=ax,\n",
        "    **common_params,\n",
        ")\n",
        "\n",
        "### Contributions Chart\n",
        "\n",
        "#Random Forest- Contributions Chart\n",
        "\n",
        "from treeinterpreter import treeinterpreter as ti\n",
        "import matplotlib.pyplot as plt\n",
        "import waterfall_chart\n",
        "\n",
        "# Escoge una instancia para analizar\n",
        "instance = x_important.iloc[0]\n",
        "\n",
        "# Calcula las contribuciones de las características\n",
        "prediction, bias, contributions = ti.predict(rf_reg_important, instance.values.reshape(1, -1))\n",
        "\n",
        "# Prepara los datos para el gráfico de cascada\n",
        "features = x_important.columns\n",
        "contributions = contributions[0]\n",
        "data = [(features[i], contribution) for i, contribution in enumerate(contributions)]\n",
        "\n",
        "# Crea el gráfico de cascada\n",
        "waterfall_chart.plot([x[0] for x in data], [x[1] for x in data], rotation_value=45, formatting='{:,.2f}')\n"
      ],
      "metadata": {
        "id": "uuVE10mwlTtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------------\n",
        "-----------------------------------------------------\n",
        "-----------------------------------------------------"
      ],
      "metadata": {
        "id": "SJmbK6RekxSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Segundo problema de Machine Learning**"
      ],
      "metadata": {
        "id": "DyLdHXMIYgA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un problema sencillo que se puede abordar con los datos proporcionados es la de definir si una tienda esta participando de una promocion en un dia dado o no. Esto se puede plantear como un problema de clasificación binaria, donde utilizaremos un conjunto de variables para entrenar un modelo de Random Forest Classifier.\n",
        "\n",
        "Variables a utilizar:\n",
        "Las detalladas en el documento Store, Train y Test. Decidimos no incluir las de la informacion externa (como por ejemplo Statestore) en nuestra version final, ya que habiendolo hecho en un principio las variables no resultaban relevantes para el modelo (lo pudimos verificar en el analisis de Feature Importance).\n",
        "\n",
        "La precisión del modelo se evalúa utilizando la métrica de accuracy_score y se presenta la matriz de confusión para evaluar el rendimiento del modelo en términos de falsos positivos, falsos negativos, verdaderos positivos y verdaderos negativos. Además, se muestra el informe de clasificación que proporciona métricas como precision, recall, f1-score y support para cada clase."
      ],
      "metadata": {
        "id": "oFPSWPC6YbGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Dividir los datos en características (x) y etiquetas (y)\n",
        "x_train = train_merged.drop(columns=['Sales', 'Date', 'Promo'])\n",
        "y_train = train_merged['Promo']\n",
        "x_test = test_merged.drop(columns=['Date', 'Promo'])\n",
        "y_test = test_merged['Promo']  # Definir y_test\n",
        "\n",
        "# Crear el modelo de clasificación binaria\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "# Entrenar el modelo\n",
        "clf.fit(x_train, y_train)\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred = clf.predict(x_test)\n",
        "# Calcular la precisión y la matriz de confusión\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "confusion_mtx = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mtx)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Grafico\n",
        "feature_importance = clf.feature_importances_\n",
        "feature_names = x_train.columns\n",
        "# Ordenar características de menor a mayor importancia\n",
        "sorted_indices = np.argsort(feature_importance)\n",
        "sorted_feature_names = feature_names[sorted_indices]\n",
        "sorted_feature_importance = feature_importance[sorted_indices]\n",
        "# Crear el gráfico de barras horizontales\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.barh(sorted_feature_names, sorted_feature_importance)\n",
        "plt.xlabel(\"Importancia\")\n",
        "plt.ylabel(\"Característica\")\n",
        "plt.title(\"Importancia de las Características\")\n",
        "plt.xticks(fontsize=8)  # Ajustar tamaño de letra en el eje x\n",
        "plt.yticks(fontsize=8)  # Ajustar tamaño de letra en el eje y\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gYonJrpylp8B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}